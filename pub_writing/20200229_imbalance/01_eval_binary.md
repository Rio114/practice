# 二値分類の評価

二値分類とは、それぞれのデータ点のある属性が正であるかか負であるかを分類するものである。例えば、ある疾患を検査する指標で閾値より大きければ罹患していると推定、小さければ罹患していないと推定して分類するといった手法である。多くの場合では予測が常に正しいとは限らない。疾患の例では体質や環境のばらつきのため特に閾値近辺では誤りが発生しやすい。そのため、二値分類を行う場合、どの程度正しく予測されてるかを評価する必要がある。


二値分類においては予測の誤りは二種類にわけられる。冒頭の疾患の例で言うと、以下のような状況である。
- 誤検知：罹患だと推定したにもかかわらず罹患していなかった
- 見逃し：罹患していないと推定したにもかかわらず罹患していた

当然ながら、誤検知も見逃しも可能な限り減らしたい。これらの誤りはトレードオフであり、検査を緩くすることで誤検知は減るが見逃しが増え、検査を厳しくすることで見逃しは減るが誤検知が増えると考えられる。極端な例では、全て罹患していると診断すると見逃しはなくなり、全て罹患していないとすれば誤検知はなくなる。しかし、これがナンセンスであることは誰の目にも明らかだろう。

二種類の誤りの重要性が互いに異なることがある。もし疾患が後から重篤化することがわかっていて、早期発見でそれが防げるとしたら、見逃しは可能な限り避けなくてはならない。検査を厳しくすることで見逃しを減らすことができるが、誤検知の可能性を増やすことにもなる。この場合、見逃しや誤検知に対する損失を定量的に評価できれば、ある閾値を決めた時の全体の損失を予測できるようになり、その損失を最小にする閾値を設定することができる。

ここでは二値分類の手法は一旦置いて、何らかの手法で二値分類を行った後の評価の仕方について整理する。

## 混合行列
二値分類を考えるとき、正負について二つの見方がある。真実について分類する場合と、予測について分類する場合である。

- 真実で分類
  - 正例：真実がPositive
  - 負例：真実がNegative
- 予測で分類
  - 正の予測：予測がPositive
  - 負の予測：予測がNegative

![母集団を二分割](pics/二分割.png "母集団を二分割")

つまり、正例・負例、予測の正負の組み合わせの2x2でデータ数を4マスに振り分けることで表を作ることができる。真実でも予測で正としたものをPositive、負としたものをNegativeとする。さらに、予測が真実と同じであればTrue、予測が真実と異なればFalseとする。例えば、負例に正の予測をしてしまうことを、False Positiveという。False Positiveは偽陽性や誤検知と言ったりもする。以降ではなるべく理解がしやすいように言葉を選んで議論する。他の組み合わせについては、以下の表にまとめた。

![混合行列](pics/混合行列.png "混合行列")

## 評価指標

一般的に精度というと、正例は正、負例は負とどれだけ正確に予測できているかという指標を思い浮かべることが多いかもしれない。しかし、不均衡なデータがあった場合、正負にかかわらず全て多数側であると予測を行うことで見かけ上高い正解率を達成できてしまう。二値分類では何か特別なものをその他大勢から選別するという目的が多く、全てをその他大勢と言ってしまっては意味が無い。そのため、二値分類では以下に示すように様々な指標を用いて評価を行う。

- Accuracy
- Precision
- Recall
- FPR
- f1スコア

以降ではこれらの指標が下記の二通りの二値分類がどのように評価されるかを見て、指標の癖を掴みたい。

- パターンA：閾値が厳しいため、正と予測される数が少ない。
- パターンB：閾値が緩いため、正と予測される数が多い。

|パターンA|正の予測  |負の予測  |合計|
|---|---|---|---|
|**正例**  |90|10|100|
|**負例**  |100|9900|10000|
|**合計**  |190|9910|10100|

|パターンB|正の予測  |負の予測  |合計|
|---|---|---|---|
|**正例**  |99|1|100|
|**負例**  |1000|9000|10000|
|**合計**  |1099|9001|10100|

#### Accuracy

```
Accuracy ＝ (TP + TN) / (TP + FP + TN + FN)
```

![Accuracy](pics/Accuracy.png "Accuracy")

一般的に言う精度であり、機械学習の文脈では正解率という。正例は正、負例は負とということをどれだけ正確に言い当てているかという指標である。前述の通り、不均衡なデータに対しては有用な指標とは言えない。

```
Accuracy_A = (90 + 9900) / 10100 = 0.9891

Accuracy_B = (99 + 9000) / 10100 = 0.9009
```

閾値を厳しくとったパターンAでは、パターンBよりも負例を言い当てる割合が大きくなるため正解率が上がっている。正例を90当てるか99当てるかよりも、負例を9900当てるか9000当てるかの方が正解率に対する影響が大きい。

#### Precision

精度、正例の言い当て率

```
Precision ＝TP / (TP + FP)
```

![Precision](pics/Precision.png "Precision")

簡単に言うと正例の言い当て率であり誤検知の小ささを示す指標である。TP+FPという分母は正の予測の合計であり、分子は正の予測の中の正例の数なので、正例の言い当て率だといえる。閾値を厳しくしてごく少数の真に正である例を正だと予測するなら間違いは少なくなりPrecisionは1に近くなる。逆に閾値を緩くしていくと、Precisionは元の集団に含まれる正例の割合に近づいていく。

```
Precision_A = 90 / (90 + 100) = 0.4738
Precision_B = 99 / (99 + 1000) = 0.0901
```

閾値を厳しくとったパターンAでは言い当てた数（TP）がパターンBよりも少ないが、Precisionで比較するとパターンAの方が大きくなっている。これは厳しく見ることで誤検知（FP）を減らしているからである。

#### Recall(TPR)

```
Recall ＝TP / (TP + FN)
```

![Recall](pics/Recall.png "Recall")

簡単に言うと正例の捕捉率であり、正例の何割を集めることができたかという指標である。。TPR（True Positive Rate）や再現率とも言う。TP+FNという分母は正例の合計であり、分子は正例の中で正と予測されたものであるため、正例の捕捉率だと言える。正解ラベルがあれば、閾値を動かすことによってRecallは0から1まで任意の値をとることができる。極端なことを言えば、どんなデータも全部正であると予測すればRecallは１になる。つまり、高すぎるRecallの下では、間違えて正とする例（FP）が増えてPrecisionが下がるため二値分類適用時の要件をよく考える必要がある。

```
Recall_A = 90 / 100 = 0.9000
Recall_B = 99 / 100 = 0.9900
```

Recallは分母が正例全体であるため閾値に依らず、この場合では100である。閾値を厳しくとったパターンAでは、正例の言い当て（TP）が少なくなるためRecallはパターンBよりも小さくなっている。

#### FPR

![FPR](pics/FPR.png "FPR")

簡単に言うと負例の取りこぼし率。単独ではあまり使うことは無いが、モデルを評価するためROC曲線を描くために算出される。Recallでは正例のうちの正の予測の割合であったが、こちらは負例のうちの正の予測の割合である。閾値を上げてRecallを上げる（＝正例の捕捉率を上げる）負例を誤って正と予測してしまう割合が増える、つまりFPRが上がる。

#### F1スコア
```
F1_score = 2 x Precision x Recall / (Precision + Recall)
逆数で表すと、
1 / F1_score = (1 / Precision + 1 / Recall) / 2
```

F1スコアはPrecisionとRecallの調和平均である。定義式に対してPrecision、Recallの定義式を代入すると、以下のようになる。
```
1 / F1_score = 1 + FP / 2 / TP + FN / 2 / TP
```
誤検知（FP）と見逃し（FN）を減らすとF1スコアの逆数が小さくなる、すなわち、F1スコアが大きくなる。加えて、誤検知が1件増えるのと見逃しが1件増えるのはF1スコアに対して同等に作用するのが式から読み取ることができる。従って、誤検知と見逃しの重要性に偏りがある場合には使えない。また、負例に偏っている不均衡データの場合、誤検知が増えるためPrecisionによる影響が大きいとも言える。

```
F1_score_A = 2 x 0.4737 x 0.9000 / (0.4737 + 0.9000) = 0.6406
F1_score_B = 2 x 0.0901 x 0.9900 / (0.0901 + 0.9900) = 0.1638
```

パターンAとパターンBを比較すると、Recallは10%程度の差異だがPrecisionは5倍を超える差異になっている。その結果、Precisionの差異に引っ張られる形でF1_scoreにも差異が見られる。


### 評価指標の数値例
これまで見てきた数字を並べて見比べて、どういった特徴があるのかを見てみたい。パターンA、パターンBはそれぞれ以下のような状況を想定していた。

- パターンA：閾値が厳しいため、正と予測される数が少ない。
- パターンB：閾値が緩いため、正と予測される数が多い。

それぞれの評価指標を見比べると以下のようになる。

|  |パターンA  |パターンB |
|---|---|---|
|正解率  |0.9891|0.9009|
|Precision（精度、正例の言い当て率）  |0.4738|0.0901|
|Recall（再現率、正例の捕捉率）  |0.9000|0.9900|
|FPR  |　0.0100|0.1000　|
|F1スコア  |　0.6406|0.1638|

ここで着目したいのはPrecisionとRecallがトレードオフになっているということである。では、どちらを見たらよいのか。それを議論するためにはドメイン知識を使って何を重視するかを決めなくてはならない。

## 損失行列
これまで見てきたように、評価指標がいくつもあると結局何を見たらよいかという問題に直面する。ここでは具体例を用いて何を重視するかを導き出す方法を考えたい。

損失行列は、混合行列の4つの事象に対してどれだけの損失（もしくは利益）が得られるかを表したものである。それぞれの損失がどの程度になるかはドメイン知識が必要であるが、考え方は以下の例のようにシンプルである。ここではコストとしてマイナスの場合は損失を表し、プラスの場合は利益を表す。

![損失行列](pics/損失行列.png "損失行列")

図のようにデータ点一つ当たりの損失を混合行列のセル一つ一つに定義してあれば、セルごとにかけ合わせることにより、全体の損失がどれくらいになるかを簡単に計算できる。

### 疾患の検査

とある疾患では治療が遅れることにより、治療のコストが飛躍的に増大する。一方で、早期発見できれば治療のコストは小さい。検査という二値分類を考えた場合の損失行列を設計する。

この検査による一人あたりのコストを以下のように仮定する。
- 検査で陽性となった場合は、真実がどうであれ再検査を受ける。再検査のコストは-10とする。
- 再検査はコストをかけるので、確実な診断ができる。すなわち、FPもFNも0で正解率100％とする。
- 再検査で罹患が確かめられた場合、治療に-90のコストを要する。
- 真実は罹患しているにもかかわらず最初の検査で発見できなかった場合（FNの場合）、将来的なコストは-2000とする。
- 罹患しておらず、検査でも陰性となった場合（TNの場合）、追加のコストは発生しない。

これらの条件を損失行列に落とし込むと以下のようになる。TPのコストは再検査と治療で100となる。

|損失行列  |正の予測  |負の予測  |
|---|---|---|
|**正例**  |-100|-2000|
|**負例**  |-10|0|

損失行列の各数値は一人あたりの損失なので混合行列の各数値と掛け合わせることで以下のように合計損失を求めることができる。

|パターンA |正の予測  |負の予測  |合計|
|---|---|---|---|
|**正例**  |-9000|-20000|-29000|
|**負例**  |-1000|0|-1000|
|**合計**  |-9000|-21000|-30000|

|パターンB |正の予測  |負の予測  |合計|
|---|---|---|---|
|**正例**  |-9900|-2000|-11900|
|**負例**  |-10000|0|-10000|
|**合計**  |-19900|-2000|-21900|

合計損失はパターンAで-30000、パターンBで-21900となる。従って、損失（の絶対値）が小さいパターンBのほうが好ましいと言える。この場合、見逃し（FN）に大きな損失が設定されているため、できるだけ見逃しをなくすという戦略が機能するためである。言い換えると、見逃しの損失が大きい損失行列であるため、Recallが大きいパターンBが有利になる。

### 広告の配信

あるサービス利用者に新サービスに関する広告の配信を考えていて、利用者ごとに新サービスの加入申込みの予測を行いたい。広告を配信したにもかかわらず申込みがなければ広告配信の費用が無駄になるが、申込みがあれば利益が見込まれる。

この広告配信による一人あたりのコストを以下のように仮定する。
- 新サービスに加入すると予測した利用者に対しては、広告配信を行うので-10のコストが発生する。
- 実際に新サービスに加入した利用者からは110の利益が得られる。
- 加入しないと予測したユーザーに対しては利益も損失も発生しない。

これらの条件を損失行列に落とし込むと以下のようになる。

|損失行列  |正の予測  |負の予測  |
|---|---|---|
|**正例**  |100|0|
|**負例**  |-10|0|

また、この場合の合計損失は以下の通りである。

|パターンA|正の予測  |負の予測  |合計|
|---|---|---|---|
|**正例**  |9000|0|9000|
|**負例**  |-1000|0|-1000|
|**合計**  |8000|0|8000|

|パターンB |正の予測  |負の予測  |合計|
|---|---|---|---|
|**正例**  |9900|0|9900|
|**負例**  |-10000|0|-10000|
|**合計**  |-100|0|-100|

合憲損失はパターンAで8000（利益）、パターンBで-100（損失）となった。当然、利益のほうが目的に適うので今度はパターンAのほうが好ましいという結果になった。この場合、見逃し（FN）には損失が無く、誤検知（FP）にのみ損失が設定されているため、Precisionが高いパターンAが有利になる。


## 多クラス分類の評価指標
クラス分類の問題で、YesかNoの二値ではなく、A, B, C,,,といった多クラスの分類を行う必要がある場合もある。可能であればA(yes)とA以外(No)といった二値分類に帰着させると課題設定がシンプルになる。しかし、そうはいかない場合、混合行列を多クラスに拡張させて考察していくことになる。

### 多クラスの混合行列
真実も予測もクラスの数だけあるので、クラス数xクラス数の大きさの行列になる。次にクラスがA, B, Cの3クラスである予測の混合行列を示す。予測の間違いについて、例えばFB_Aとしたときに、真実はAであるがBと予測してしまったFalseと表記する。

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |TA|FB_A|FC_A|
|**真実がB**  |FA_B|TB|FC_B|
|**真実がC**  |FA_C|FB_C|TC|

もし、A, B, Cの分類においてB, Cの区別が重要ではなくAかA以外かの二値分類で十分な場合、多クラスの混合行列から二値分類の混合行列は直ちに得られる。

|混合行列  |予測がA  |予測がA以外  |
|---|---|---|
|**真実がA**  |TA  |FB_A + FC_A |
|**真実がA以外**  |FA_B + FA＿C |TB + TC ＋FB_C + FC_B|

### 多クラスの評価指標

評価指標も二値分類と同様に考えることができる。

#### Accuracy
AのものはA、BのものはB、CのものはCとどれだけ正確に予測できているかという指標である。二値分類でのAccuracyと同様に不均衡なデータでは多数を占めるクラスの影響が大きい。また、後に示すようにPrecisionやRecallのマイクロ平均に一致する。

```
Accuracy = (TA + TB + TC) / (TA + FB_A + FC_A + TB + FA_B + FC_B + TC + FA_C + FB_C)
```

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|***FB_A***|***FC_A***|
|**真実がB**  |***FA_B***|***TB***|***FC_B***|
|**真実がC**  |***FA_C***|***FB_C***|***TC***|

#### Precision
例えばAについて着目したとき、Aと予測したもののうち真にAであるものの割合、という指標になる。

```
Precision_A = TA / (TA + FA_B + FA_C)
```

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|FB_A|FC_A|
|**真実がB**  |***FA_B***|TB|FC_B|
|**真実がC**  |***FA_C***|FB_C|TC|

#### Recall
例えばAについて着目したとき、真実がAのものをどれだけ捕捉できたか、という指標になる。

```
Recall_A = TA / (TA + FB_A + FC_A)
```

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|***FB_A***|***FC_A***|
|**真実がB**  |FA_B|TB|FC_B|
|**真実がC**  |FA_C|FB_C|TC|

- マクロ平均

クラスごとに計算した評価指標の平均。もしクラス間で数が不均衡である場合、数が少ないクラスの評価指標を、数が多いクラスと同等に扱ってよいかはそれぞれの分析で個別に判断しなくてはならない。

- ミクロ平均

ミクロ平均は割り算を行うときに全て考慮してしまう。マクロ平均と異なり、クラス間で不均衡な場合は多数クラスの結果に引っ張られる。

```
Recall_micro = (TA + TB + TC) / (TA + FB_A + FC_A + TB + FA_B + FC_B + TC + FA_C + FB_C)
```
