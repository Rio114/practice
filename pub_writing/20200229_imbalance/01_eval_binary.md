# 二値分類の評価

二値分類とは、それぞれのデータ点のある属性が正であるかか負であるかを分類するものである。例えば、ある疾患を検査する指標で閾値より大きければ罹患していると推定、小さければ罹患していないと推定して分類するといった手法である。多くの場合では予測が常に正しいとは限らない。疾患の例では体質や環境のばらつきのため特に閾値近辺では誤りが発生しやすい。そのため、二値分類を行う場合、どの程度正しく予測されてるかを評価する必要がある。

二値分類においては、予測の誤りは二種類にわけられる。冒頭の疾患の例で言うと、以下のような状況である。
- 罹患だと推定したにもかかわらず罹患していなかった
- 罹患していないと推定したにもかかわらず罹患していた

分析を行う分野によっては、二種類の誤りの重要性が互いに異なることがある。もし疾患が後から重篤化することがわかっていて、早期発見でそれが防げるとしたら、罹患の見逃しは可能な限り避けなくてはならない。それは検査を厳しくするということにもつながるが、罹患していない人にも罹患であるという診断を下すことになり



この章では何らかの方法で二値分類を行った結果を評価する方法を整理したい。


## 混合行列
二値分類を考えるとき、正負について二つの見方がある。真実は正負のどちらか、予測は正負のどちらかである。つまり、真実と予測の組み合わせの2x2でデータ数を4マスに振り分けることで表を作ることができる。真実でも予測で正としたものをPositive、負としたものをNegativeとする。さらに、予測が真実と同じであればTrue、予測が真実と異なればFalseとする。これを表に表すと次のようになる。

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP (True Positive、真陽性)  |FN (False Negative、偽陰性) |
|**真実がNegative**  |FP (False Positive、偽陽性) |TN (True Negative、真陰性) |

この検査の正しさを評価するとき、検査指標が閾値よりも大きいにも関わらず罹患していない。


## 評価指標

一般的に精度というと、真実が正のものは正、真実が負のものは負とどれだけ正確に分類できているかという指標である。しかし、二値分類ではこの指標が重視されることが少ない。なぜなら不均衡なデータがあった場合、正負にかかわらずすべて同じ予測を行うことで正解率を上げることができてしまうからである。二値分類では何か特別なものをその他大勢から選別するという目的が多く、すべてをその他大勢と言ってしまっては意味が無い。そのため、二値分類では以下に示すように様々な指標を用いて評価を行う。

- Accuracy：正解率
- Precision：言い当て率
- Recall (TPR)：正の捕捉率
- FPR：負の取りこぼし率
- f1：PrecisionとRecallの調和平均

#### Accuracy
> Accuracy ＝ (TP + TN) / (TP + FP + TN + FN)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP|FN|
|**真実がNegative**  |FP|TN|

一般的に言う精度であり、真実が正のものは正、真実が負のものは負とどれだけ正確に分類できているかという指標である。前述の通り、不均衡なデータに対しては有用な指標とは言えない。

#### Precision
> Precision ＝TP / (TP + FP)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |***TP***|FN|
|**真実がNegative**  |***FP***|TN|

上の二つと同じように簡単に言うと正例の言い当て率。分母の二つは両方とも正だと予測したものの数であり、分子はそのうち真に正であるものの数なので正例の言い当て率だといえる。閾値を厳しくしてごく少数の真に正である例を正だと予測するなら間違いは少なくなりPrecisionは1に近くなる。逆に閾値を緩くしていくと、Precisionは元々のサンプルに含まれる正の例の割合に近づいていく。

#### Recall(TPR)
> Recall = TP / ( TP + FN)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |***TP***|***FN***|
|**真実がNegative**  |FP|TN|

簡単に言うと正例の捕捉率。いろんな説明が巷にあふれているが、個人的にはこの言葉でスッと入ると思う。TP+FNという分母は両方とも真に正のデータの数であり、分子は正であると予測されたものである。つまり、予測によって正例を何割を集めることができたかという指標となる。正解ラベルがあれば、閾値を動かすことによって０～１まで任意に設定できる。極端なことを言えば、どんなデータも全部正であると予測すればTPRは１になる。つまり、TPRを上げ過ぎると、間違えて正とする例が増えてくるため適用例の要件をよく考える必要がある。

*べん図で説明*

#### FPR
> FPR = FP / (FP + TN)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP|FN|
|**真実がNegative**  |***FP***|***TN***|

こちらは負例の取りこぼし率。単独ではあまり使うことは無いが、モデルを評価するためROC曲線を描くために算出される。TPRでは真に正であるものを集めた割合であったが、こちらは真に負であるものをどれだけ取りこぼしたかという割合。閾値を上げてTPRを上げる（＝正例の捕捉率を上げる）と負例の取りこぼしは多くなっていく。TPR=1なら負例もすべて正としてしまうので負例はすべて取りこぼされる、ということでFPR=1となる。

*べん図で説明*

## 損失行列
評価指標がいくつもあると、結局何を見たらよいか、という問題に直面する。ここではドメイン知識を使って、何を重視するかを導き出す方法を考えたい。

例としてある疾患の検査を挙げる。罹患しているかしていないかはある検査指標の閾値を元に判断されるが、誤って陽性としたり、誤って陰性としたりしてしまう。

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP|FN|
|**真実がNegative**  |FP|TN|



## 多クラス分類の評価指標
クラス分類の問題で、YesかNoの二値ではなく、A, B, C,,,といった多クラスの分類を行う必要がある場合もある。可能であればA(yes)とA以外(No)といった二値分類に帰着させると課題設定がシンプルになる。しかし、そうはいかない場合混合行列を多クラスに拡張させて考察していくことになる。

### 多クラスの混合行列
真実も予測もクラス数あるので、クラス数xクラス数の大きさの行列になる。次にクラスがA, B, Cの3クラスである予測の混合行列を示す。予測の間違いについて、例えばFB_Aとしたときに、真実はAであるがBと予測してしまったFalseと表記する。

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |TA|FB_A|FC_A|
|**真実がB**  |FA_B|TB|FC_B|
|**真実がC**  |FA_C|FB_C|TC|

もし、A, B, Cの分類においてB, Cの区別が重要ではなくAかA以外かの二値分類で十分な場合、多クラスの混合行列から二値分類の混合行列は直ちに得られる。

|混合行列  |予測がA  |予測がA以外  |
|---|---|---|
|**真実がA**  |TA  |FB_A + FC_A |
|**真実がA以外**  |FA_B + FA＿C |TB + TC ＋FB_C + FC_B|

### 多クラスの評価指標

評価指標も二値分類と同様に考えることができる。

#### Accuracy
AのものはA、BのものはB、CのものはCとどれだけ正確に予測できているかという指標である。二値分類でのAccuracyと同様に不均衡なデータでは多数を占めるクラスの影響が大きい。また、後に示すようにPrecisionやRecallのマイクロ平均に一致する。

> Accuracy = (TA + TB + TC) / (TA + FB_A + FC_A + TB + FA_B + FC_B + TC + FA_C + FB_C)

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|***FB_A***|***FC_A***|
|**真実がB**  |***FA_B***|***TB***|***FC_B***|
|**真実がC**  |***FA_C***|***FB_C***|***TC***|

#### Precision
例えばAについて着目したとき、Aと予測したもののうち真にAであるものの割合、という指標になる。

> Precision_A = TA / (TA + FA_B + FA_C)

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|FB_A|FC_A|
|**真実がB**  |***FA_B***|TB|FC_B|
|**真実がC**  |***FA_C***|FB_C|TC|

#### Recall
例えばAについて着目したとき、真実がAのものをどれだけ捕捉できたか、という指標になる。

> Recall_A = TA / (TA + FB_A + FC_A)

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|***FB_A***|***FC_A***|
|**真実がB**  |FA_B|TB|FC_B|
|**真実がC**  |FA_C|FB_C|TC|

- マクロ平均

クラスごとに計算した評価指標の平均。もしクラス間で数が不均衡である場合、数が少ないクラスの評価指標を、数が多いクラスと同等に扱ってよいかはそれぞれの分析で個別に判断しなくてはならない。

- ミクロ平均

ミクロ平均は割り算を行うときに全て考慮してしまう。マクロ平均と異なり、クラス間で不均衡な場合は多数クラスの結果に引っ張られる。

> TPR_micro = (TA + TB + TC) / (TA + FB_A + FC_A + TB + FA_B + FC_B + TC + FA_C + FB_C)
