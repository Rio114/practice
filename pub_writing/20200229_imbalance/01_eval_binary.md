# 二値分類の評価

ここではまず何らかの方法で二値分類を行った結果を評価する方法を整理したい。

例えば、ある検査指標で閾値より大きければ罹患、小さければ罹患していないという分類を行うことを考える。この検査の正しさを評価するとき、検査指標が閾値よりも大きいにも関わらず罹患していない

## 混合行列
二値分類を考えるとき、正負について二つの見方がある。真実は正負のどちらか、予測は正負のどちらかである。つまり、真実と予測の組み合わせの2x2でデータ数を4マスに振り分けることで表を作ることができる。真実でも予測で正としたものをPositive、負としたものをNegativeとする。さらに、予測が真実と同じであればTrue、予測が真実と異なればFalseとする。これを表に表すと次のようになる。

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP (True Positive)  |FN (False Negative) |
|**真実がNegative**  |FP (False Positive) |TN (True Negative) |

## 評価指標
- Accuracy：正解率
- Precision：言い当て率
- Recall (TPR)：正の捕捉率
- FPR：負の取りこぼし率
- f1：PrecisionとRecallの調和平均


#### TPR (Recall)
TPR = TP / ( TP + FN)

簡単に言うと正例の捕捉率。いろんな説明が巷にあふれているが、個人的にはこの言葉でスッと入ると思う。分母は両方とも真に正のデータの数であり、分子は正であると予測されたものである。つまり、予測によって正例を何割を集めることができたかという指標。正解ラベルがあれば、閾値を動かすことによって０～１まで任意に設定できる。極端なことを言えば、どんなデータも全部正であると予測すればTPRは１になる。つまり、TPRを上げ過ぎると、間違えて正とする例が増えてくるため適用例の要件をよく考える必要がある。

*べん図で説明*

#### FPR
FPR = FP / (FP + TN)

こちらは負例の取りこぼし率。TPRでは真に正であるものを集めた割合であったが、こちらは真に負であるものをどれだけ取りこぼしたかという割合。閾値を上げてTPRを上げる（＝正例の捕捉率を上げる）と負例の取りこぼしは多くなっていく。TPR=1なら負例もすべて正としてしまうので負例はすべて取りこぼされる、ということでFPR=1となる。

*べん図で説明*

#### Precision
Precision ＝TP / (TP + FP)

上の二つと同じように簡単に言うと正例の言い当て率。分母の二つは両方とも正だと予測したものの数であり、分子はそのうち真に正であるものの数なので正例の言い当て率だといえる。閾値を厳しくしてごく少数の真に正である例を正だと予測するなら間違いは少なくなりPrecisionは1に近くなる。逆に閾値を緩くしていくと、Precisionは元々のサンプルに含まれる正の例の割合に近づいていく。

## 損失行列
評価指標がいくつもあると、結局何を見たらよいか、という問題に直面する。ここではドメイン知識を使って、何を重視するかを導き出す方法を考えたい。

## 多クラス分類の評価指標
クラス分類の問題で、YesかNoの二値ではなく、A, B, C,,,といった多クラスの分類を行う必要がある場合もある。可能であればA(yes)とA以外(No)といった二値分類に帰着させると課題設定がシンプルになる。しかし、そうはいかない場合混合行列を多クラスに拡張させて考察していくことになる。

### 多クラスの混合行列
真実も予測もクラス数あるので、クラス数xクラス数の大きさの行列になる。次にクラスがA, B, Cの3クラスである予測の混合行列を示す。予測の間違いについて、例えばFB_Aとしたときに、真実はAであるがBと予測してしまったFalseと表記する。

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |TA|FB_A|FC_A|
|**真実がB**  |FA_B|TB|FC_B|
|**真実がC**  |FA_C|FB_C|TC|

評価指標も二値分類と同様に考えることができる。

#### TPR (Recall)
例えばAについて着目したとき、真実がAのものをどれだけ捕捉できたか、という指標になる。

TPR_A = TA / (TA + FB_A + FC_A)


- マクロ平均

クラスごとに計算した評価指標の平均。もしクラス間で数が不均衡である場合、数が少ないクラスの評価指標を、数が多いクラスと同等に扱ってよいかはそれぞれの分析で個別に判断しなくてはならない。

- ミクロ平均

ミクロ平均は割り算を行うときに全て考慮してしまう。マクロ平均と異なり、クラス間で不均衡な場合は多数クラスの結果に引っ張られる。

TPR_micro = (TA + TB + TC) / (TA + FB_A + FC_A + TB + FA_B + FC_B + TC + FA_C + FB_C)



# モデルの評価
## 良いモデルとは
- 完全モデル

- ポンコツモデル

## スコアの評価
### 順序
- Recall-Precision曲線
- ROC曲線

### 絶対値
- logloss
