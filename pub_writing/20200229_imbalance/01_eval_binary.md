# 二値分類の評価

二値分類とは、それぞれのデータ点のある属性が正であるかか負であるかを分類するものである。例えば、ある疾患を検査する指標で閾値より大きければ罹患していると推定、小さければ罹患していないと推定して分類するといった手法である。多くの場合では予測が常に正しいとは限らない。疾患の例では体質や環境のばらつきのため特に閾値近辺では誤りが発生しやすい。そのため、二値分類を行う場合、どの程度正しく予測されてるかを評価する必要がある。

二値分類においては予測の誤りは二種類にわけられる。冒頭の疾患の例で言うと、以下のような状況である。
- 誤検知：罹患だと推定したにもかかわらず罹患していなかった
- 見逃し：罹患していないと推定したにもかかわらず罹患していた

当然ながら、誤検知も見逃しも可能な限り減らしたい。これらの誤りはトレードオフであり、検査を緩くすることで誤検知は減るが見逃しが増え、検査を厳しくすることで見逃しは減るが誤検知が増えると考えられる。極端な例では、全て罹患していると診断すると見逃しはなくなり、全て罹患していないとすれば誤検知はなくなる。しかし、これがナンセンスであることは誰の目にも明らかだろう。

二種類の誤りの重要性が互いに異なることがある。もし疾患が後から重篤化することがわかっていて、早期発見でそれが防げるとしたら、見逃しは可能な限り避けなくてはならない。検査を厳しくすることで見逃しを減らすことができるが、誤検知の可能性を増やすことにもなる。この場合、見逃しや誤検知に対する損失を定量的に評価できれば、ある閾値を決めた時の全体の損失を予測できるようになり、その損失を最小にする閾値を設定することができる。

ここでは二値分類の手法は一旦置いて、何らかの手法で二値分類を行った後の評価の仕方について整理する。

## 混合行列
二値分類を考えるとき、正負について二つの見方がある。真実は正負のどちらか、予測は正負のどちらかである。つまり、真実と予測の組み合わせの2x2でデータ数を4マスに振り分けることで表を作ることができる。真実でも予測で正としたものをPositive、負としたものをNegativeとする。さらに、予測が真実と同じであればTrue、予測が真実と異なればFalseとする。これを表に表すと次のようになる。冒頭で議論した誤検知や見逃しはそれぞれ、False Positive、False Negativeに当たる。

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP (True Positive、真陽性)  |FN (False Negative、偽陰性、見逃し) |
|**真実がNegative**  |FP (False Positive、偽陽性、誤検知) |TN (True Negative、真陰性) |

## 評価指標

一般的に精度というと、真実が正のものは正、真実が負のものは負とどれだけ正確に分類できているかという指標である。しかし、二値分類ではこの指標が重視されることが少ない。なぜなら不均衡なデータがあった場合、正負にかかわらずすべて同じ予測を行うことで見かけ上高い正解率を達成できてしまうからである。二値分類では何か特別なものをその他大勢から選別するという目的が多く、すべてをその他大勢と言ってしまっては意味が無い。そのため、二値分類では以下に示すように様々な指標を用いて評価を行う。

- Accuracy：正解率
- Precision：精度(言い当て率)
- Recall (TPR)：再現率(正の捕捉率)
- FPR：負の取りこぼし率
- f1：PrecisionとRecallの調和平均

以降ではこれらの指標が下記の二通りの二値分類がどのように評価されるかを見て、指標の癖を掴みたい。

|パターンA：緩い閾値  |予測がPositive  |予測がNegative  |合計|
|---|---|---|---|
|**真実がPositive**  |90|10|100|
|**真実がNegative**  |100|9900|10000|
|**合計**  |190|9910|10100|

|パターンB：厳しい閾値  |予測がPositive  |予測がNegative  |合計|
|---|---|---|---|
|**真実がPositive**  |99|1|100|
|**真実がNegative**  |1000|9000|10000|
|**合計**  |1099|9001|10100|

#### Accuracy
> Accuracy ＝ (TP + TN) / (TP + FP + TN + FN)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP|FN|
|**真実がNegative**  |FP|TN|

一般的に言う精度であり、真実が正のものは正、真実が負のものは負とということをどれだけ正確に言い当てているかという指標である。前述の通り、不均衡なデータに対しては有用な指標とは言えない。

> Accuracy_A = (90 + 9900) / 10100 = 0.9891
>
> Accuracy_B = (99 + 9000) / 10100 = 0.9009

#### Precision
> Precision ＝TP / (TP + FP)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |***TP***|FN|
|**真実がNegative**  |***FP***|TN|

上の二つと同じように簡単に言うと正例の言い当て率であり誤検知の小ささを示す指標である。分母の二つは両方とも正だと予測したものの数であり、分子はそのうち真に正であるものの数なので正例の言い当て率だといえる。閾値を厳しくしてごく少数の真に正である例を正だと予測するなら間違いは少なくなりPrecisionは1に近くなる。逆に閾値を緩くしていくと、Precisionは元々のサンプルに含まれる正の例の割合に近づいていく。

> Precision_A = 90 / (90 + 100) = 0.4738
>
> Precision_B = 99 / (99 + 1000) = 0.0901

#### Recall(TPR)
> Recall = TP / ( TP + FN)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |***TP***|***FN***|
|**真実がNegative**  |FP|TN|

簡単に言うと正例の捕捉率。いろんな説明が巷にあふれているが、個人的にはこの言葉でスッと入ると思う。TP+FNという分母は両方とも真に正のデータの数であり、分子は正であると予測されたものである。つまり、予測によって正例を何割を集めることができたかという指標となる。正解ラベルがあれば、閾値を動かすことによって０～１まで任意に設定できる。極端なことを言えば、どんなデータも全部正であると予測すればTPRは１になる。つまり、TPRを上げ過ぎると、間違えて正とする例が増えてくるため適用例の要件をよく考える必要がある。

> Recall_A = 90 / 100 = 0.9000
>
> Recall_B = 99 / 100 = 0.9900

*べん図で説明*

#### FPR
> FPR = FP / (FP + TN)

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP|FN|
|**真実がNegative**  |***FP***|***TN***|

こちらは負例の取りこぼし率。単独ではあまり使うことは無いが、モデルを評価するためROC曲線を描くために算出される。TPRでは真に正であるものを集めた割合であったが、こちらは真に負であるものをどれだけ取りこぼしたかという割合。閾値を上げてTPRを上げる（＝正例の捕捉率を上げる）と負例の取りこぼしは多くなっていく。TPR=1なら負例もすべて正としてしまうので負例はすべて取りこぼされる、ということでFPR=1となる。

*べん図で説明*

#### F1スコア
> F1_score = 2 x Precision x Recall / (Precision + Recall)

逆数で表すと、
> 1 / F1_score = (1 / Precision + 1 / Recall) / 2

F1スコアはPrecisionとRecallの調和平均である。定義式に対してPrecision、Recallの定義式を代入すると、以下のようになり誤検知と見逃しを減らすとF1スコアの逆数が小さくなる、すなわち、F1スコアを大きくできるということになる。加えて、誤検知が1件増えるのと見逃しが1件増えるのはF1スコアに対して同等に作用するのが式から読み取れる。従って、誤検知と見逃しのどちらかを重視する場合には使えない。また、負例に偏っている不均衡データの場合、FPが大きな数字になるためPrecisionによる影響が大きくなるとと読み取れる。

> 1 / F1_score = 1 + FP / 2 / TP + FN / 2 / TP

### 評価指標の数値例
これまで見てきた数字を並べて見比べて、どういった特徴があるのかを見てみたい。

|  |緩い閾値 |厳しい閾値 |
|---|---|---|
|正解率  |0.9891|0.9009|
|Precision（精度、言い当て率）  |0.4738|0.0901|
|Recall（再現率、正の捕捉率）  |0.9000|0.9900|
|FPR  |　0.0100|0.1000　|
|F1スコア  |　0.6406|0.1638|

## 損失行列
評価指標がいくつもあると、結局何を見たらよいか、という問題に直面する。ここでは具体例を用いて何を重視するかを導き出す方法を考えたい。

損失行列は、混合行列の4つの事象に対してどれだけの損失（もしくは利益）が得られるかを表したものである。それぞれの損失がどの程度になるかはドメイン知識が必要であるが、考え方は以下の例のようにシンプルである。ここではコストとしてマイナスの場合は損失を表し、プラスの場合は利益を表す。

### 疾患の検査

とある疾患では治療が遅れることにより、治療のコストが飛躍的に増大する。一方で、早期発見できれば治療のコストは小さい。検査という二値分類を考えた場合の損失行列を設計する。

この検査による一人あたりのコストを以下のように仮定する。
- 検査で陽性となった場合は、真実がどうであれ再検査を受ける。再検査のコストは-10とする。
- 再検査はコストをかけるので、確実な診断ができる。すなわち、FPもFNも0で正解率100％とする。
- 再検査で罹患が確かめられた場合、治療に-90のコストを要する。
- 真実は罹患しているにもかかわらず最初の検査で発見できなかった場合（FNの場合）、将来的なコストは-2000とする。
- 罹患しておらず、検査でも陰性となった場合（TNの場合）、追加のコストは発生しない。

これらの条件を損失行列に落とし込むと以下のようになる。TPのコストは再検査と治療で100となる。

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |-100|-2000|
|**真実がNegative**  |-10|0|

損失行列の各数値は一人あたりの損失なので混合行列の各数値と掛け合わせることで以下のように合計損失を求めることができる。

|パターンA：緩い閾値  |予測がPositive  |予測がNegative  |合計|
|---|---|---|---|
|**真実がPositive**  |-9000|-20000|-29000|
|**真実がNegative**  |-1000|0|-1000|
|**合計**  |-9000|-21000|-30000|

|パターンB：厳しい閾値  |予測がPositive  |予測がNegative  |合計|
|---|---|---|---|
|**真実がPositive**  |-9900|-2000|-11900|
|**真実がNegative**  |-10000|0|-10000|
|**合計**  |-19900|-2000|-21900|

合計損失は緩い閾値のパターンAで-30000、パターンBで-21900となる。従って、損失（の絶対値）が小さいパターンBのほうが好ましいと言える。この場合、見逃し（FN）に大きな損失が設定されているため、できるだけ見逃しをなくすという戦略が機能するためである。言い換えると、見逃しの損失が大きい損失行列であるため、Recallが大きいパターンBが有利になる。

### 広告の配信

あるサービス利用者に新サービスに関する広告の配信を考えていて、利用者ごとに新サービスの加入申込みの予測を行いたい。広告を配信したにもかかわらず申込みがなければ広告配信の費用が無駄になるが、申込みがあれば利益が見込まれる。

この広告配信による一人あたりのコストを以下のように仮定する。
- 新サービスに加入すると予測した利用者に対しては、広告配信を行うので-10のコストが発生する。
- 実際に新サービスに加入した利用者からは110の利益が得られる。
- 加入しないと予測したユーザーに対しては利益も損失も発生しない。

これらの条件を損失行列に落とし込むと以下のようになる。

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |100|0|
|**真実がNegative**  |-10|0|

また、この場合の合計損失は以下の通りである。

|パターンA：緩い閾値  |予測がPositive  |予測がNegative  |合計|
|---|---|---|---|
|**真実がPositive**  |9000|0|9000|
|**真実がNegative**  |-1000|0|-1000|
|**合計**  |8000|0|8000|

|パターンB：厳しい閾値  |予測がPositive  |予測がNegative  |合計|
|---|---|---|---|
|**真実がPositive**  |9900|0|9900|
|**真実がNegative**  |-10000|0|-10000|
|**合計**  |-100|0|-100|

合憲損失はパターンAで8000（利益）、パターンBで-100（損失）となった。当然、利益のほうが目的に適うので今度はパターンAのほうが好ましいという結果になった。この場合、見逃し（FN）には損失が無く、言い間違い（FP）にのみ損失が設定されているため、Precisionが高いパターンAが有利になる。


## 多クラス分類の評価指標
クラス分類の問題で、YesかNoの二値ではなく、A, B, C,,,といった多クラスの分類を行う必要がある場合もある。可能であればA(yes)とA以外(No)といった二値分類に帰着させると課題設定がシンプルになる。しかし、そうはいかない場合混合行列を多クラスに拡張させて考察していくことになる。

### 多クラスの混合行列
真実も予測もクラス数あるので、クラス数xクラス数の大きさの行列になる。次にクラスがA, B, Cの3クラスである予測の混合行列を示す。予測の間違いについて、例えばFB_Aとしたときに、真実はAであるがBと予測してしまったFalseと表記する。

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |TA|FB_A|FC_A|
|**真実がB**  |FA_B|TB|FC_B|
|**真実がC**  |FA_C|FB_C|TC|

もし、A, B, Cの分類においてB, Cの区別が重要ではなくAかA以外かの二値分類で十分な場合、多クラスの混合行列から二値分類の混合行列は直ちに得られる。

|混合行列  |予測がA  |予測がA以外  |
|---|---|---|
|**真実がA**  |TA  |FB_A + FC_A |
|**真実がA以外**  |FA_B + FA＿C |TB + TC ＋FB_C + FC_B|

### 多クラスの評価指標

評価指標も二値分類と同様に考えることができる。

#### Accuracy
AのものはA、BのものはB、CのものはCとどれだけ正確に予測できているかという指標である。二値分類でのAccuracyと同様に不均衡なデータでは多数を占めるクラスの影響が大きい。また、後に示すようにPrecisionやRecallのマイクロ平均に一致する。

> Accuracy = (TA + TB + TC) / (TA + FB_A + FC_A + TB + FA_B + FC_B + TC + FA_C + FB_C)

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|***FB_A***|***FC_A***|
|**真実がB**  |***FA_B***|***TB***|***FC_B***|
|**真実がC**  |***FA_C***|***FB_C***|***TC***|

#### Precision
例えばAについて着目したとき、Aと予測したもののうち真にAであるものの割合、という指標になる。

> Precision_A = TA / (TA + FA_B + FA_C)

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|FB_A|FC_A|
|**真実がB**  |***FA_B***|TB|FC_B|
|**真実がC**  |***FA_C***|FB_C|TC|

#### Recall
例えばAについて着目したとき、真実がAのものをどれだけ捕捉できたか、という指標になる。

> Recall_A = TA / (TA + FB_A + FC_A)

|混合行列  |予測がA|予測がB|予測がC|
|---|---|---|---|
|**真実がA**  |***TA***|***FB_A***|***FC_A***|
|**真実がB**  |FA_B|TB|FC_B|
|**真実がC**  |FA_C|FB_C|TC|

- マクロ平均

クラスごとに計算した評価指標の平均。もしクラス間で数が不均衡である場合、数が少ないクラスの評価指標を、数が多いクラスと同等に扱ってよいかはそれぞれの分析で個別に判断しなくてはならない。

- ミクロ平均

ミクロ平均は割り算を行うときに全て考慮してしまう。マクロ平均と異なり、クラス間で不均衡な場合は多数クラスの結果に引っ張られる。

> TPR_micro = (TA + TB + TC) / (TA + FB_A + FC_A + TB + FA_B + FC_B + TC + FA_C + FB_C)
