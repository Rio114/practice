機械学習の応用として、二値分類は割とポピュラーなテーマです。例えば、顧客情報を元にサービス加入予測やCT予測を行うといった場面。
ところがそういったものは100人に一人加入なりCTすれば御の字で、1％未満の正例を予測するという難しいタスクになりがちです。


[:contents]




## 課題
- 不均衡データの学習
- モデルによるスコアの意味
- モデル性能の比較

## 不均衡データの学習
1％未満の正例を予測するモデルの学習では、正例となるデータの説明変数の特徴を捉えることは難しい。
サンプリングによって均衡なデータにして学習を行う、言い換えると正例を重視するとスコアが偏る。

## モデルによるスコアの意味
素朴に考えると二値分類モデルのスコア=期待CTRとなるはずだが、両者の散布図を描いてみるとそうとも言えないことがわかる。

*santenderの例*

学習時にはloglossを使っているため、数式に立ち戻って考えたい。

## モデル性能の評価
スコアが高いほど正例となる確率は高いということはどうやら正しそうだが、モデルの性能を定量的に評価して比較しようとするとやや複雑な見方をしなくてはならない。ここで予測の精度とモデルの性能という言葉遣いを見直しておきたい。
ここでは二値分類を考えているため、正負の予測を行うためには連続値であるスコアをある閾値で分ける必要がある。予測の精度はある閾値に対して、スコアがそれより上のとき正、下のとき負という予測ラベルを与えたときの正解の割合である。さらに面倒くさいことに、分母の取り方で精度の解釈が異なってくる。

巷にはいろんな説明があり、ずらっと並んでいるとどれがどれだかわからない。ここでは少し違った説明をしようと思う。理解のコツは、捕捉できたか取りこぼしたか、言い当てたか言い間違えたかという二つの観点を持つことである。それを示すのがConfusion Matrixである。

### 予測精度
#### Confusion Matrix (混合行列)
二値分類を考えるとき、二つの見方がある。真実は正負のどちらか、予測は正負のどちらかである。つまり、真実と予測の組み合わせの2x2でデータ数を4マスに振り分けることで表を作ることができる。真実でも予測で正としたものをPositive、負としたものをNegativeとする。さらに、予測が真実と同じであればTrue、予測が真実と異なればFalseとする。

|混合行列  |予測がPositive  |予測がNegative  |
|---|---|---|
|**真実がPositive**  |TP (True Positive)  |FN (False Negative) |
|**真実がNegative**  |FP (False Positive) |TN (True Negative) |

#### TPR (Recall)
TPR = TP / ( TP + FN)

簡単に言うと正例の捕捉率。いろんな説明が巷にあふれているが、個人的にはこの言葉でスッと入ると思う。分母は両方とも真に正のデータの数であり、分子は正であると予測されたものである。つまり、予測によって正例を何割を集めることができたかという指標。正解ラベルがあれば、閾値を動かすことによって０～１まで任意に設定できる。極端なことを言えば、どんなデータも全部正であると予測すればTPRは１になる。つまり、TPRを上げ過ぎると、間違えて正とする例が増えてくるため適用例の要件をよく考える必要がある。

*べん図で説明*

#### FPR
FPR = FP / (FP + TN)

こちらは負例の取りこぼし率。TPRでは真に正であるものを集めた割合であったが、こちらは真に負であるものをどれだけ取りこぼしたかという割合。閾値を上げてTPRを上げる（＝正例の捕捉率を上げる）と負例の取りこぼしは多くなっていく。TPR=1なら負例もすべて正としてしまうので負例はすべて取りこぼされる、ということでFPR=1となる。

*べん図で説明*

#### Precision
Precision ＝TP / (TP + FP)

上の二つと同じように簡単に言うと正例の言い当て率。分母の二つは両方とも正だと予測したものの数であり、分子はそのうち真に正であるものの数なので正例の言い当て率だといえる。閾値を厳しくしてごく少数の真に正である例を正だと予測するなら間違いは少なくなりPrecisionは1に近くなる。逆に閾値を緩くしていくと、Precisionは元々のサンプルに含まれる正の例の割合に近づいていく。

*べん図で説明*

### モデル性能

そもそも良いモデル、悪いモデルというのはどういうものなのだろうか。

良いモデル：完全モデル。スコアがそのまま予測確率になっているモデルである。つまり、同等なスコアの集団を集めてきた場合、正例の数がスコアをそのまま母数ｐとする二項分布に従うというモデルである。この場合、スコアとスコアで層別化した正例率をプロットすると直線に乗り、相関係数は１になる。

現実的なモデル：現実にはスコアと層別化正例率は概ね相関があるものの直線に乗ることは少ない。

悪いモデル：ポンコツモデル。正解ラベルがなんであっても二値分類スコアとして０－１の一様乱数を出力するモデルである。前述のスコアと層別化正例率の相関が無いというものである。

自分で閾値を決めなくてならない予測精度に対して、モデルの性能そのものが良いのか悪いのかを判断したり比較したいことがよくある。そういったときによく用いられるのがROC曲線やPR曲線、それぞれを一つの数字に落とし込んだAUCやAPである。

忙しい人はAUCが0.8くらいで良いモデル、0.9だと過学習なんじゃないのと判断してしまうことがよくあるが、結局自分の考えている領域内の比較で良し悪しを判断すべきである。ビッグで不均衡なデータだと多様性により学習が困難となり、AUCが0.7に満たないモデルで運用していることもある。

AUC、APは順序性しか見ていないため、何らかのシステムでスコア値を下流で処理に用いる場合には注意が必要がある。例えば、二つのモデルのスコアを比較して高いほうを採用するといった文脈付きバンディット問題を考える場合にはスコアの絶対値が比較可能かを検証しておくべきである。

スコアの絶対値を直接評価する場合はLoglossを使う。数式的には、、

#### ROC曲線
Receiver Operatorating Characteristic curve（受信者動作特性曲線)
左のほうは閾値が厳しい、右に行くと緩くなる。下記のAUCが同じなら左のほうの立ち上がりが良いほうが良いモデルである。なぜならその場合、FPRをあまり上げずにTPRを上げることができているからである。
ROC曲線では正負の割合がわからない。
正負ともに分類の数を元に議論をする場合には便利である。例えば、ある病気の検査で偽陽性、つまり本当は病気ではないのに病気であると判断されることに何らかの損失が考えられる場合、偽陽性の数の増減により全体の損失がどのように変動するかを議論できる。

- AUC
ROCの下側面積Area Under Curve。

#### Recall-Precision曲線
不均衡データだと左のほうでギザギザする。右に行くと閾値を緩くすることになるのでPrecisionは下がる。
RP曲線は不均衡データが一目瞭然
正の言い当てを重視する場合にはROCよりも便利である。例えば、真に正を半分集める（Recallが0.5）ためには30％の言い当て率（Precisionが0.3）で我慢しなくてはならないといった具合である。この際、負例には興味がなく、スコアの最大値をとるようなバンディット問題では有用だと言える。

### AP
Recall-Precision curveの下側面積であり、スコアの閾値を色々変えたときのPrecision

### logloss

### スコア絶対値
スコアの順序の評価
