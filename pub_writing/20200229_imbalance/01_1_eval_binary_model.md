# 二値分類モデルの評価

二値分類モデルは様々な形で構成できる。例えば、あるデータ点に対して対象となる事象発生の予測確率の大きさを示す値が得られるとき、適当な閾値で集団を分けて閾値より大きな予測値あのものは正、閾値より小さい予測値のものを負と予測することで二値分類を構成できる。また、ルールベースによりデータの持つ属性がある条件を満たすときに正と予測するとしても二値分類を構成できる。

後者はより一般的な場合であり条件ごとに混合行列を検証しなくてはならないが、前者は条件が予測値という一次元の数値で表されているので閾値を一次元的に動かすだけで連続的に評価できる。

従って、ここでは前者の評価方法を整理する。この場合、閾値を決めることで各データ点が混合行列のどこに入るかが決まる。つまりPrecisionやRecallなどの指標は閾値次第で分類の結果が変わるので、閾値に依存しない形でモデルの評価を行いたい。

そこでよく用いられるのがROC曲線やPR曲線、そしてそれらをそれぞれ一つの数字に落とし込んだものがAUCやAPである。なお、ROC曲線やPR曲線を作成するときには、モデルの予測値は確率のように0から1の範囲である必要はないが、以下では簡単のためモデルの予測値は0から1の範囲の予測確率を示すものとする。

## 良いモデルとは

そもそも良いモデル、悪いモデルというのはどういうものなのだろうか。一旦はあまり深く考えずに以下のようなモデルがありそうだということが想像できる。

- 神モデル：森羅万象を司る神様のように事象が起こるなら1、起こらないなら0を出力して、正解率100%を達成する、事象の発生を確実に予言できるモデル。実用の場面ではまずあり得ないモデルなので、考えないことにする。

- ベルヌーイ試行モデル：予測値がそのまま予測確率になっているモデル。つまり、同等な予測値の集団を集めてきた場合、正例の数がスコアをそのまま母数ｐとするベルヌーイ試行に従うというモデルである。この場合、スコアとスコアで層別化した正例率をプロットすると直線に乗り、相関係数は１になる。

- ポンコツモデル：正解ラベルがなんであっても二値分類スコアとして0から1の一様乱数を出力するモデル。前述のスコアと層別化正例率の相関が無い。

- 現実的なモデル：現実にはスコアと層別化正例率は概ね相関があるものの直線に乗ることは少ない。ベルヌーイ試行モデルとポンコツモデルの間の存在である。

- 天邪鬼モデル：発生しそうだと低い予測値、発生しにくいだろうというときに高い予測値を返すモデル。予測を反転させれば現実的なモデルになる。

モデルを評価する指標としてAUCというのがあるが、忙しい人はAUCが0.8くらいで良いモデル、0.9だと過学習なんじゃないのと判断してしまうことがよくある。しかし、ビッグで不均衡なデータだと多様性により学習が困難となり、AUCが0.7に満たないモデルで運用していることもある。結局、精度は単体では意味をなさず、必ず何を重視するのかを決めたうえで、相対比較を行っていくことが客観的な評価となる。

また、AUCはスコアの順序性しか見ていないため、何らかのシステムでスコアの数値を下流で処理に用いる場合には注意が必要がある。例えば、二つのモデルのスコアを比較して高いほうを採用するといった文脈付きバンディット問題を考える場合にはスコアの絶対値が比較可能かを検証しておくべきである。

## ROC曲線
二値分類では、負例の中から正と予測した割合よりも正例の中から正と予測した割合のほうが大きくなることが望まれる。そのトレードオフがどのようになっているかを視覚化したのがReceiver Operating Characteristic curve（受信者動作特性曲線)を略してROC曲線である。負例を誤って正と予測する割合（FPR）に対して、正例をどれだけの割合で捕捉できるか（TPR、Recall）ということを示す。負例をすべて間違いなく負と予測したければ、全てを負と予測すれば達成できるが、その時は正例を全く捕捉できない。逆に、正例を全て捕捉しようと全てを正と予測すれば、全ての負例が正と予測されてしまう。出来るだけ正例を多く捕捉しつつ、負例を正と予測する割合は押さえたい。

### 曲線の描き方

スコアの順番にデータ点を並べて、閾値を動かしていった時に、x軸にFPR、y軸にTPRを取ったものがROC曲線である。

*書き方の図示*

### 曲線の特徴
閾値が厳しいものだと母集団の多くを負であると予測してしまうため、正例の捕捉率は小さく負例の取りこぼしは少ない。逆に閾値を緩くすると正例の捕捉率は高くなるが、負例を誤って正と予測してしまう数が増える。従って、曲線は右肩上がりになる。

通常、二値分類モデルがある程度学習できているのであれば、負例の中から誤って正としてしまう割合よりも、正例の中から正しく正を捕捉できる割合の方が大きい。逆にポンコツなモデルを考えると、正例の中からも負例の中からも等しい割合で正の予測を行ってしまう。従って、ROC曲線は上に凸で斜め45度線の上側に弓なりである曲線となる。

*正例と負例のそれぞれで正と予測する割合、長さの違う棒を二色に分ける*

スコアは順番に並べることができれば確率値のように0から1の範囲である必要はない。逆に言うと、ROC曲線はスコアの順序性を反映しているものの、絶対値に関する情報を読み取ることができない。

#### ROC曲線では正負の割合がわからない。
正負ともに分類の数を元に議論をする場合には便利である。例えば、ある病気の検査で偽陽性、つまり本当は病気ではないのに病気であると判断されることに何らかの損失が考えられる場合、偽陽性の数の増減により全体の損失がどのように変動するかを議論できる。

### AUC
ROCの下側面積Area Under Curveを略してAUCと言い、モデルの良し悪しを一つの数値で表すことができる。これはTPR、すなわちRecallの平均と見ることができるので、モデルによる正例の捕捉をどれだけ期待できるかを示している。

ポンコツなモデルだとAUCは0.5になる。一方、一つの閾値で正負を分けることができればAUCは1になる。

AUCが同じなら左のほうの立ち上がりが良いほうが良いモデルである。なぜならその場合、FPRをあまり上げずにTPRを上げることができているからである。

## Recall-Precision曲線
ROC曲線の時とは異なり、正例に焦点を当てて考えて評価を行うこともできる。つまり、正例を多く捕捉しつつ、正の予測ではなるべく多くを言い当てたい。ROC曲線とのニュアンスの違いが難しいが、こちらでは負例の数についてはあまり興味がない状況を念頭に置いている。そういった目的のために、二値分類スコアの閾値に対して得られる様々なRecallとPrecisionの組み合わせをプロットしたのがRecall-Precision曲線、略してRP曲線である。これにより、正例の捕捉割合を上げることで 正の予測の正しさがどれだけ下がるかのトレードオフを視覚化できる。

### 曲線の描き方

スコアの順番にデータ点を並べて、閾値を動かしていった時に、x軸にRecall、y軸にPrecisionを取ったものがRecall-Precision曲線である。

*書き方の図示*

## AP（RP曲線のAUC）
RP曲線でもAUCを考えることができる。これはROCのAUCと区別してAverage Precision、略してAPという。名前の通り、APはモデルによる正例の言い当て率がどれくらい期待できるかということを示している。

ポンコツなモデルでは、モデルで予測を行う前の集団の正例の割合となる。一方、一つの閾値で正負を分けることができればAPは1になる。

### 曲線の特徴
正例を多く捉えるために閾値を緩くすることで、負例を誤って正としてしまう（FP）数が増えるので通常右肩下がりの曲線となる。ただし、特に不均衡データのモデル学習が足りない場合、左のほうでPrecisionの分母となる正の予測数（TP+FP）が少ないため変動が大きい。

*正例について捕捉できた割合と言い当てた割合、四角を十字で分けた図*

RP曲線もROC曲線と同様に、スコアは順番に並べることができれば確率値のように0から1の範囲である必要はない。逆に言うと、RP曲線はスコアの順序性を反映しているものの、絶対値に関する情報を読み取ることができない。


#### RP曲線は不均衡データが一目瞭然
正の言い当てを重視する場合にはROCよりも便利である。例えば、正例を半分集める（Recallが0.5）ためには30％の言い当て率（Precisionが0.3）で我慢しなくてはならないといった具合である。この際、負例には興味がなく、スコアの最大値をとるようなバンディット問題では有用だと言える。

## スコア散布図
ROCやRPはスコア順にデータ点を並べて集計することにより作った。つまり、スコアの絶対値は評価されていない。しかし、二値分類スコアそのものがどの程度、正例を予測しているかという、スコアの絶対値に興味がある場合がある。それぞれのデータ点は0か1のいずれかであり、x軸にスコア、y軸に正負をそのままプロットしても解釈が難しい。そこで、スコアの近いものをまとめて（例えば、0.01刻みで層別化する）、正負の0－1フラグの平均を代表させることで、スコアが二項分布の確率母数を表すと解釈する。そのようにスコアと予測確率を一対一に対応させることができれば、例えば異なるモデル間でスコアを比較し、その大小で判断を行うようなバンディット問題に応用できる。これを散布図としてプロットすれば視覚的にモデルの良し悪しを検証できる。

*書き方の図示*

### logloss
異なるモデル間でのスコアの比較を考えると、同じスコアは同じ予測確率を示すことが前提となる。それは散布図では同一のグラフに乗ることを意味しており、特に数値の解釈性の観点から、斜め45度の直線に乗るようなスコア＝予測確率となることが望ましい。

都合の良いことに、説明変数が同じでスコアが同一になると考えられる集団があったとすると、二項分布の最尤推定を行うことでスコア＝予測確率となる。これは形式的にはloglossを最小化することと同等である。二値分類予測の損失関数としてloglossが用いられることが多いのはこういった理由がある。

*y=0, 1のそれぞれに点が集まっている図*

*y=0, 1のそれぞれに点が集まっている図、あるスコアで層別したときに割合を計算する図*


## 評価例
冒頭で挙げたモデルに対して、それぞれのモデル評価がどのようになるかを見比べたい。

||視覚化|指標|
|---|---|---|
|Recall|ROC曲線|AUC|
|Precision|RP曲線|AP|
|絶対値|散布図|logloss|
