{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1, 0, 0, 0],\n",
    "                   [0, 1, 1, 0],\n",
    "                  [1, 0, 1, 0],\n",
    "                  [0, 1, 0, 1]])\n",
    "y_pred = np.array([[0.6, 0.1, 0.25, 0.05],\n",
    "                   [0.55, 0.3, 0.05, 0.1],\n",
    "                  [0.4, 0.45, 0.05, 0.1],\n",
    "                  [0.55, 0.3, 0.05, 0.1]])\n",
    "\n",
    "tf_y_true = tf.constant(y_true)\n",
    "tf_y_pred = tf.constant(y_pred)\n",
    "\n",
    "truth = K.placeholder(shape=[None, 4])\n",
    "score = K.placeholder(shape=[None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    print(\"num_classes:\", num_classes)\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    print(\"pos_class_indices:\",pos_class_indices)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    print(\"retrieved_classes:\", retrieved_classes)\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    print('class_rankings:', class_rankings)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    print('class_rankings:', class_rankings)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    print('retrieved_class_true:', retrieved_class_true)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    print('retrieved_class_true:', retrieved_class_true)\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    print('retrieved_cumulative_hits:', retrieved_cumulative_hits)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        print(\"pos_class_indices, precision_at_hits:\", pos_class_indices, precision_at_hits)\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return sum(per_class_lwlrap*weight_per_class), per_class_lwlrap, weight_per_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 4\n",
      "pos_class_indices: [0]\n",
      "retrieved_classes: [0 2 1 3]\n",
      "class_rankings: [0 0 0 0]\n",
      "class_rankings: [0 2 1 3]\n",
      "retrieved_class_true: [False False False False]\n",
      "retrieved_class_true: [ True False False False]\n",
      "retrieved_cumulative_hits: [1 1 1 1]\n",
      "pos_class_indices, precision_at_hits: [0] [1.]\n",
      "num_classes: 4\n",
      "pos_class_indices: [1 2]\n",
      "retrieved_classes: [0 1 3 2]\n",
      "class_rankings: [0 0 0 0]\n",
      "class_rankings: [0 1 3 2]\n",
      "retrieved_class_true: [False False False False]\n",
      "retrieved_class_true: [False  True False  True]\n",
      "retrieved_cumulative_hits: [0 1 1 2]\n",
      "pos_class_indices, precision_at_hits: [1 2] [0.5 0.5]\n",
      "num_classes: 4\n",
      "pos_class_indices: [0 2]\n",
      "retrieved_classes: [1 0 3 2]\n",
      "class_rankings: [0 0 0 0]\n",
      "class_rankings: [1 0 3 2]\n",
      "retrieved_class_true: [False False False False]\n",
      "retrieved_class_true: [False  True False  True]\n",
      "retrieved_cumulative_hits: [0 1 1 2]\n",
      "pos_class_indices, precision_at_hits: [0 2] [0.5 0.5]\n",
      "num_classes: 4\n",
      "pos_class_indices: [1 3]\n",
      "retrieved_classes: [0 1 3 2]\n",
      "class_rankings: [0 0 0 0]\n",
      "class_rankings: [0 1 3 2]\n",
      "retrieved_class_true: [False False False False]\n",
      "retrieved_class_true: [False  True  True False]\n",
      "retrieved_cumulative_hits: [0 1 2 2]\n",
      "pos_class_indices, precision_at_hits: [1 3] [0.5        0.66666667]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5952380952380951,\n",
       " array([0.75      , 0.5       , 0.5       , 0.66666667]),\n",
       " array([0.28571429, 0.28571429, 0.28571429, 0.14285714]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_per_class_lwlrap(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_one_sample_positive_class_precisions(y_true, y_pred) :\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    \n",
    "    # find true labels\n",
    "    pos_class_indices = tf.where(y_true > 0) \n",
    "    \n",
    "    # put rank on each element\n",
    "    class_rankings = tf.nn.top_k(y_pred, k=num_classes).indices \n",
    "    \n",
    "    #pick_up ranks\n",
    "    num_correct_until_correct = tf.gather_nd(class_rankings, pos_class_indices) \n",
    "    \n",
    "    # add one for division for \"presicion_at_hits\"\n",
    "    num_correct_until_correct_one = tf.add(num_correct_until_correct, 1) \n",
    "    num_correct_until_correct_one = tf.cast(num_correct_until_correct_one, tf.float32)\n",
    "    \n",
    "    # generate tensor [num_sample, predict_rank], \n",
    "    # top-N predicted elements have flag, N is the number of positive for each sample.\n",
    "    sample_label = pos_class_indices[:, 0]   \n",
    "    sample_label = tf.reshape(sample_label, (-1, 1))\n",
    "    sample_label = tf.cast(sample_label, tf.int32)\n",
    "    num_correct_until_correct = tf.reshape(num_correct_until_correct, (-1, 1))  \n",
    "    retrieved_class_true_position = tf.concat((sample_label, \n",
    "                                               num_correct_until_correct), axis=1)\n",
    "    retrieved_pos = tf.ones(shape=tf.shape(retrieved_class_true_position)[0], dtype=tf.int32)\n",
    "    retrieved_class_true = tf.scatter_nd(retrieved_class_true_position, \n",
    "                                         retrieved_pos, \n",
    "                                         tf.shape(y_pred))\n",
    "\n",
    "    # cumulate predict_rank\n",
    "    retrieved_cumulative_hits = tf.cumsum(retrieved_class_true, axis=1)\n",
    "\n",
    "    # find positive position\n",
    "    pos_ret_indices = tf.where(retrieved_class_true > 0)\n",
    "\n",
    "    # find cumulative hits\n",
    "    correct_rank = tf.gather_nd(retrieved_cumulative_hits, pos_ret_indices)  \n",
    "    correct_rank = tf.cast(correct_rank, tf.float32)\n",
    "\n",
    "    # compute presicion\n",
    "    precision_at_hits = tf.truediv(correct_rank, num_correct_until_correct_one)\n",
    " \n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def tf_lwlrap(y_true, y_pred):\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    \n",
    "    pos_class_indices, precision_at_hits = (tf_one_sample_positive_class_precisions(y_true, y_pred))\n",
    "    \n",
    "    pos_flgs = tf.cast(y_true > 0, tf.int32)\n",
    "    \n",
    "    labels_per_class = tf.reduce_sum(pos_flgs, axis=0)\n",
    "    \n",
    "    weight_per_class = tf.truediv(tf.cast(labels_per_class, tf.float32),\n",
    "                                  tf.cast(tf.reduce_sum(labels_per_class), tf.float32))\n",
    "    \n",
    "    sum_precisions_by_classes = tf.zeros(shape=(num_classes), dtype=tf.float32)  \n",
    "    \n",
    "    class_label = pos_class_indices[:,1]\n",
    "\n",
    "    sum_precisions_by_classes = tf.unsorted_segment_sum(precision_at_hits,\n",
    "                                                        class_label,\n",
    "                                                       num_classes)\n",
    "    \n",
    "    labels_per_class = tf.cast(labels_per_class, tf.float32)\n",
    "    labels_per_class = tf.add(labels_per_class, 1e-7)\n",
    "    per_class_lwlrap = tf.truediv(sum_precisions_by_classes,\n",
    "                                  tf.cast(labels_per_class, tf.float32))\n",
    "    \n",
    "    out = tf.cast(tf.tensordot(per_class_lwlrap, weight_per_class, axes=1), dtype=tf.float32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5952381"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf_lwlrap(tf_y_true, tf_y_pred)\n",
    "sess = tf.Session()\n",
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5952381"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf_lwlrap(truth, score)\n",
    "sess = tf.Session()\n",
    "sess.run(x,  feed_dict={truth:y_true, score:y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
